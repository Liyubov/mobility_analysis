{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymization of data \n",
    "\n",
    "Here we first anonymize the data from researchers using:\n",
    "1. simple cutting precision of data \n",
    "2. randomizing time-samples (and keeping exact time stamps in another file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data functions\n",
    "\n",
    "# function to get dataframe from json \n",
    "\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "'''\n",
    "Functions:\n",
    " to estimate the distribution of stop duration\n",
    " to estimate the distribution of jump length\n",
    "'''\n",
    "\n",
    "def duration_stop_distribution(daily_segments):\n",
    "    \"\"\"\n",
    "    takes a daily segment of Moves \n",
    "    returns sorted distribution of stops duration\n",
    "        Can be misleading for days w/ lots of travel etc. \n",
    "    \"\"\"\n",
    "    places_of_day = []\n",
    "    duration_seq = [] #sequence of durations of stops\n",
    "    for i in daily_segments:\n",
    "        if i['type'] == 'place':\n",
    "            place_location = i['place']['location']\n",
    "            start_time = datetime.strptime(i['startTime'],'%Y%m%dT%H%M%S%z')\n",
    "            end_time = datetime.strptime(i['endTime'],'%Y%m%dT%H%M%S%z')\n",
    "            duration = end_time - start_time\n",
    "            duration_seq.append([duration])\n",
    "    duration_seq.sort() \n",
    "    return duration_seq \n",
    "\n",
    "def len_jumps_distribution(daily_segments):\n",
    "    \"\"\"\n",
    "    takes a daily segment of Moves \n",
    "    returns the distribution of jumps lengths (calculated from lat/long of stops)\n",
    "    can be misleading for days w/ lots of travel etc. \n",
    "    \"\"\"\n",
    "    places_of_traj = [] #returns array of all locations during the day\n",
    "    for i in daily_segments: \n",
    "        if i['type'] == 'place':\n",
    "            place_location = i['place']['location']\n",
    "            places_of_traj.append([place_location])\n",
    "    return places_of_traj\n",
    "\n",
    "\n",
    "def longest_daily_location(daily_segments):\n",
    "    \"\"\"\n",
    "    Function from Bastian (Open Humans)\n",
    "    \n",
    "    Bastian did analysis of most popular location data in his notebook for Open humans\n",
    "    https://exploratory.openhumans.org/notebooks/?source=Moves%20connection\n",
    "    \n",
    "    takes a daily segment log of Moves and returns the \n",
    "    lat/long for the location where most time was spent. \n",
    "    Can be misleading for days w/ lots of travel etc. \n",
    "    But the most quick/dirty solution for now.\n",
    "    \"\"\"\n",
    "    places_of_day = []\n",
    "    for i in daily_segments:\n",
    "        if i['type'] == 'place':\n",
    "            place_location = i['place']['location']\n",
    "            start_time = datetime.strptime(i['startTime'],'%Y%m%dT%H%M%S%z')\n",
    "            end_time = datetime.strptime(i['endTime'],'%Y%m%dT%H%M%S%z')\n",
    "            duration = end_time - start_time\n",
    "            places_of_day.append([place_location,duration])\n",
    "    places_of_day.sort(key=lambda tup: tup[-1],reverse=True)\n",
    "    return places_of_day[0][0]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "function to create new dataframe from the json trajectory file\n",
    "code adapted from openhumans notebook\n",
    "'''\n",
    "\n",
    "def dataframe_from_json(moves_data):\n",
    "    for datapoint in moves_data:\n",
    "        # we need to have observed segments for that day. If moves wasn't running we ignore the day\n",
    "        if datapoint['segments'] != None:\n",
    "            # did we stay in a place that day and did we walk that day?\n",
    "            has_places = False\n",
    "            walked = False\n",
    "            for i in datapoint['segments']:\n",
    "                if i['type'] == 'place':\n",
    "                        # yes, we were in one place w/o moving around too much, we can keep this day\n",
    "                        has_places = True\n",
    "                        \n",
    "            # is this day in our date range of interest and has data?\n",
    "            if datapoint['summary'] != None and has_places and datetime.strptime(datapoint['date'],\"%Y%m%d\") > datetime.strptime(DATARANGE_START,\"%Y-%m-%d\"):\n",
    "                moves_processed_data['date'].append(datapoint['date'])\n",
    "                for activity in datapoint['summary']:\n",
    "                    if activity['activity'] == 'walking':\n",
    "                        moves_processed_data['steps'].append(activity['steps'])\n",
    "                        moves_processed_data['distance'].append(activity['distance'])\n",
    "                        walked = True\n",
    "                        \n",
    "                # in case of not walking, step count is zero\n",
    "                if not walked:\n",
    "                    moves_processed_data['steps'].append(0)\n",
    "                    moves_processed_data['distance'].append(0)  \n",
    "                    \n",
    "                # distribution of stops lengths\n",
    "                stops_distrib = duration_stop_distribution(datapoint['segments'])\n",
    "                moves_processed_data['duration'].append(stops_distrib)\n",
    "                location = longest_daily_location(datapoint['segments'])\n",
    "                moves_processed_data['lat'].append(location['lat'])\n",
    "                moves_processed_data['lon'].append(location['lon'])    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Now that we have all of the data we can convert it into a single pandas dataframe for easier processing and visualization\n",
    "    \n",
    "    moves_dataframe = pd.DataFrame(data={\n",
    "        'date': moves_processed_data['date'],\n",
    "        'steps': moves_processed_data['steps'],\n",
    "        'distance': moves_processed_data['distance'],\n",
    "        'latitude': moves_processed_data['lat'],\n",
    "        'longitude': moves_processed_data['lon']\n",
    "    })   \n",
    "    \n",
    "    return moves_dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function analyzing moves_data    \n",
    "DATARANGE_START = \"2016-06-01\"\n",
    "DATARANGE_END = \"2018-05-08\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# sets the axis label sizes for seaborn\n",
    "rc={'font.size': 14, 'axes.labelsize': 14, 'legend.fontsize': 14.0, \n",
    "    'axes.titlesize': 14, 'xtick.labelsize': 14, 'ytick.labelsize': 14}\n",
    "sns.set(rc=rc)\n",
    "\n",
    "with open('C:/Users/lyubo/Documents/DATA_networks/mobilitydata/openhumans/moves-storyline-data.json') as f:\n",
    "    moves_data = json.load(f)\n",
    "    \n",
    "with open('C:/Users/lyubo/Documents/DATA_networks/mobilitydata/openhumans/moves-storyline-data98933.json') as f: #uploading different datafiles\n",
    "    moves_data2 = json.load(f)\n",
    "\n",
    "#with open('C:/Users/lyubo/Documents/DATA_networks/mobilitydata/openhumans/moves-storyline-data98972.json') as f:\n",
    "#    moves_data3 = json.load(f)   \n",
    "    \n",
    "\n",
    "# function to get data from json data\n",
    "moves_processed_data = defaultdict(list) #empty dict for processed dataframe\n",
    "\n",
    "df = dataframe_from_json(moves_data2)  \n",
    "    \n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First method of anonymisation\n",
    "\n",
    "We \"semi\"-anonymize data by cutting the lat,lon exact locations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "shape = df.shape\n",
    "print(int(shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with rounding \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lat_anon = np.zeros(int(shape[0]))\n",
    "lon_anon = np.zeros(int(shape[0]))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    #print(index)\n",
    "    lat_anon[index] = round(row['latitude'], 2)\n",
    "    lon_anon[index] = round(row['longitude'], 2)\n",
    "    #print(row['latitude'], row['longitude'])\n",
    "\n",
    "print('done with rounding of data ')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second type of anonymisation\n",
    "Anonymisation using shuffling of data\n",
    "https://medium.com/district-data-labs/a-practical-guide-to-anonymizing-datasets-with-python-faker-ecf15114c9be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
